The project that I attempted was to create an accent differentiator using a convolutional neural network structure. I did not achieve great results unfortunately, but it was still a worthwhile exercise working for the first time with audio in neural nets. The dataset I worked with was from George Mason University, which since 1999 has compiled thousands of voices from hundreds of countries. Each person says the same line, which is helpful for consistency across subjects, and each is asked 7 demographic questions to help provide accurate information about their English language history. The creators of this database are linguists and so the paragraph recited was specifically chosen to draw out as many accent quirks as possible. This all culminated to create high quality data for this project. The only drawback is that there is not as many samples as I would have hoped for. A language in this database has at most have a few hundred speakers, but never more.
	To overcome this difficulty, as well as other pieces of natural language that can confuse a neural network, a serious amount of preprocessing work was done to the training data. The first bit of code takes all of the biographical information of the target languages’ speakers. It scrapes the data from the GMU website and puts all of the information into a data frame to be used later for the outcome labels. Second, the code takes all the corresponding audio files and puts them into a folder to be used in the for the main preprocessing steps. The first piece of real preprocessing is loading the wave files of the speakers and then down-sampling them. This is done to shorten the vectors of the training data, while preserving as much of the original signals as possible. With audio, training times can be very long, so this step is helpful in shortening it. After this, the code hunts for instances of silence in the audio and omits it so that silence does not confuse the network. The next important step is to take these wave files and turn them into MFCCs. These are very typically used for audio processing. They were designed to pick up on characteristics similar to what human ear listens for. They are calculated using the amplitude, frequency, and slope of the Fourier transforms (digital representations of sound) that were produced from the recordings. Using all these characteristics, a value is formed for each point along the time axis and is plotted on a graph like below. Finally, the MFCCs are normalized which has been proven in studies to increase accuracy in results. Theses MFCCs are then chopped up into segments so that there is more training data available for the model.

	The reason MFCCs are best for this project is because they perform well without a lot of data. They also have fewer coefficients than spectrograms which make them easier to process. Based on the decision to use MFCCs in this project, a CNN was selected as the best architecture to be used. CNNs are well known to typically be the best option for working with MFCCs and spectrograms. In the figure to the left, I depict an MFCC taken from this project. As you can see, it is basically a heat map of the different values taken of the audio file. A more simplified spectrogram acts as a heat map for different frequencies in audio files. For both of these options, a CNN makes a lot of sense, because for many reasons, a MFCC acts as a picture. Choosing a model that is designed for that kind of data, therefore, makes a lot of sense. CNNs are best for images because they are spatially aware. Since objects can move around in images, this is really important. Similarly in language, inflections that show a specific accent might be at different points in the MFCC representation. The CNN is able to view these moving features by using a series of kernels or filters that goes over the image in chunks and looks for certain features attributed to that filter. A larger filter will look more broadly at images. A pooling layer is then used typically to find the most prominent features that were found from the filters. In this model, a 3x3 filter and a 2x2 max pooling layer was used. Two convolutional layers were used. 
	Overfitting was a very likely problem with this assignment, because my data was heavily skewed towards native English speakers. Overfitting occurs when the model learns the specific data too well, and trains itself with a narrow focus. This is a problem, because when you feed it new data, it won’t be very accurate. To avoid overfitting in this model, I employed both dropout and early stopping. Dropout is a really effective way to avoid overfitting, because it randomly removes connections between layers at a rate that you decide. This forces models to become more general, because relying on a few connections too much is not possible. I also used early stopping, which will stop the model from training if it detects that overfitting is occurring. It decides this by seeing how much accuracy changes between epochs and stops at a certain tolerance set by the coder.
The overall results were slightly below the baseline accuracy. You can see this result on the accuracy and loss tables above. In all, about 64% accuracy was achieved. The data was fairly skewed towards English at around 67% of all data, so it was a little worse than just guessing English each time. In the graphs you can see that the accuracy climbed pretty steadily, but early stopping forced the model to stop because it was beginning to overfit. The overfitting becomes clear when compared to the validation accuracy that originally climbs in the first 20 epochs, before staying roughly the same. The loss side, as expected, shows a similar picture. 

Confusion Matrix
4230	122	355
790	217	255
1028	214	647

Looking at the confusion matrix, we see that there is some overfitting occurring, but the network still made many attempts to guess one of the other languages. The model predicted English 76% of the time which was a little higher than I would have hoped, but I was happy to see that it was still predicting the other languages. The model definitely was best at predicted English but predicted Arabic fairly accurately also. 
	Overall, these results were a little disappointing and show that a new model might be necessary to avoid the problem of overfitting and increase accuracy. I would also like to see how the model performs when it has a more balanced dataset and more distinct data. In this case, I would be curious to see which language it overfits to if it does at all. If it continues to choose English, I would be interested to explore if there is some type of bias in the way that this architecture is built.



